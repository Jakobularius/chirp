#-*- mode: org -*-
#+OPTIONS:    H:3 num:nil toc:2 \n:nil @:t ::t |:t ^:{} -:t f:t *:t TeX:t LaTeX:t skip:t d:(HIDE) tags:not-in-toc
#+STARTUP:    align fold nodlcheck hidestars oddeven
#+TITLE:    Chirp: Analysis and Comparison of Bioacoustic Recordings
#+AUTHOR:    C Daniel Meliza
#+LANGUAGE:   en
#+BABEL: :exports code
#+LaTeX_CLASS: koma-article
#+LATEX_HEADER: \usepackage{amsmath,graphicx,hyperref}

* Installation instructions

** Prerequisites

Chirp is written almost entirely in Python, but it requires some
external packages to calculate spectrograms and geometric
transformation.  The minimum requirements are:

1. Python (VERSIONS)
2. Numpy (VERSION)
3. Shapely (VERSION); requires GEOS (VERSION)
4. libtfr (https://github.com/dmeliza/libtfrspec); requires fftw
   (http://www.fftw.org/; version 3.2+)
5. Scipy (try to eliminate)

To run the chirp GUI, these packages are also required:

1. wxPython (version)
2. matplotlib
3. pyaudio (optional; allows audio playback)

*** OS X (MacPorts)

It's fairly straightfoward to install the dependencies on an OS X
machine using MacPorts.  After installing MacPorts, run the shell
following shell commands (as root, or using sudo):

#+begin_src sh
port install python26 py26-shapely fftw-3
port install py26-numpy -atlas
easy_install libtfr
#+end_src

Replace python26 with python27 if desired. For the optional
dependencies:

#+begin_src sh
port install py26-wxpython +carbon -gtk py26-matplotlib +wxpython portaudio
CFLAGS="-I/opt/local/include" easy_install \
 http://people.csail.mit.edu/hubert/pyaudio/packages/pyaudio-0.2.4.tar.gz
#+end_src

(note that the location/version of pyaudio may change; check
http://people.csail.mit.edu/hubert/pyaudio/ if the above link doesn't work)

*** Linux

* Components

Chirp consists of a number of interlinked programs that perform
specific tasks.  This modular design makes it easier to set up batch
processing jobs and to add new functionality.

** chirp

*** Overview

The /chirp/ program is a graphical interface for examining
spectrograms of vocalizations and identifying regions of interest.
Regions may be temporal, defined by a start and stop time, or they may
be spectrotemporal, defined by a two-dimensional polygonal mask.

The interface for drawing masks is highly flexible.  Masks can be
combined, split, and trimmed.  Pitch calculations can be run in the
GUI to iteratively refine mask shapes.  Masks and intervals are stored
in /ebl/ files, a flexible format based on GIS's "Well Known Text".

*** Basic usage

To start /chirp/, run it from the command-line, optionally specifying
a wave file to inspect. The following examples will use files from the
/examples/ subdirectory.

#+begin_src sh
chirp examples/KENB_MZ000162_sono_1_124570_134767_m12.wav
#+end_src

If a file called 'chirp.cfg' is in the current directory, /chirp/ will
load configuration data from this file.  An example config file is
provided in the root of the source directory (see [[*Configuration%20file][Configuration file]]
for more information on this file).

When a new file is opened, /chirp/ will calculate the spectrogram. If
a file with a corresponding name and the /ebl/ extension exists, this
will also be loaded.  


**** Spectrogram display and navigation

On loading a wave file, the interface will probably look something
like this:

***** TODO screenshot of unmasked signal

Immediately beneath the spectrogram are controls that manipulate the
appearance of the spectrogram.  Adjust these until the signal appears
clear.

+ spectrogram type :: can be 'tfr' or 'hanning'. TFR spectrograms
     provide much sharper images but take longer to calculate. The
     pitch-tracking algorithm uses TFR spectrograms under the hood
+ window size :: controls the time-frequency resolution of the
                 spectrogram. Longer windows give better frequency
                 resolution at the expense of lower temporal
                 resolution.
+ shift :: controls the amount that the analysis window is shifted
           between frames.  Smaller numbers give more finely-spaced
           frames, which may help in visualizing time-varying signals.
+ freq range :: set what frequencies are displayed in the
                window. Only values between 0 and half the sampling
                frequency make any sense.
+ color range :: set the dynamic range of the spectrogram, relative to
                 the maximum power of the signal. Smaller values
                 restrict the display to more intense spectrotemporal
                 regions.
+ colormap :: select the system of mapping power to color.  Different
              people prefer different colors.

Initially /chirp/ will display the entire duration of the signal.
Longer signals take longer to analyze, and opening a file more than a
few seconds in length can take quite a while.  To speed things up, use
the hanning method, and increase the shift parameter.  Future versions
of /chirp/ will dynamically adjust the shift value depending on the
duration of the signal.

To zoom in on a segment of the signal, select a temporal interval by
clicking on the spectrogram with the middle mouse button and drag.
Vertical bars will indicate the selected region.  Pressing the down
arrow key will zoom in on the selected segment.  Pressing up will zoom
back to the previous viewpoint.  When zoomed, pressing left and right
arrow keys will pan the viewport across the spectrogram.

**** Selecting temporal and spectrotemporal sements

Segments identify regions of interest in the signal.  They can be
temporal or spectrotemporal. 

Temporal elements are defined by their stop and starting times, and
include all the frequencies present in the original recording.
Acoustic objects that can be segmented this way include words,
syllables, songs, and calls.  Because all frequencies are included, a
temporal segment will also include any backgroud noise present during
the recording.

To create a temporal segment, click on the spectrogram with the middle
mouse button and drag to the other endpoint.  Press 's' to save the segment.

Spectrotemporal elements are defined by a region in time-frequency
space. The shape of this region can be arbitrarily complex.  It can be
narrow to include only a few frequencies, and then broaden to include
many frequencies at a later point in the signal.  If two signals are
produced cotemporaneously, but are spectrotemporally disjoint (i.e. do
not have power at the same frequencies at the same times), they can be
uniquely specified using a spectrotemporal mask.  By carefully
defining these masks it's possible to eliminate or reduce interference
from background noise.  It may also be possible to separate the
signals produced by different sources, such as the two sides of a
bird's syrinx.

To create a spectrotemporal segment, click on the spectrogram with the
left mouse button, then move the mouse to create an outline around the
region of interest.  Click the left mouse button to close the polygon.
Press 's' to save the segment.

**** Manipulating segments

When a segment is saved with the 's' key, an entry will appear in the
listbox below the spectrogram, and the area associated with the
segment will be overlaid on the spectrogram.  The visibility of each
segment can be controlled by clicking the associated check box, or by
using the "Show All" and "Hide All" buttons.  Segments selected in
the listbox will appear with a thicker outline.

To delete one or more segments, select them in the list by clicking
(shift-click to select multiple segments), then click the Delete
button.

To merge two or more segments, select them in the list and click
Merge.  Only spectrotemporal segments can be merged, and if segments
don't overlap it's not possible to merge them.

Segments can be trimmed and split in a variety of ways to produce
complex masks.  Under the hood, the segments are defined by polygons
that can be simple or complex (i.e. with interior rings).  To remove a
region from a segment, draw another region with the left mouse button
and press the 'x' key.  The drawn region will be subtracted from all
the segments in the spectrogram.  Segments can also be directly
subtracted from each other.  Select two or more segments and press the
Subtract button.  The smaller segments will all be extracted from the
largest one.  Finally, you can select two segments and use the Split
button to divide the two polygons into mutually disjoint regions.

See [[*Mask%20design%20considerations][Mask design considerations]] for further notes on making good masks.

**** File operations

Elements can be stored to disk for further editing and for use in
later analysis steps.  Both interval and spectrotemporal elements are
stored in an /enhanced label/ (/ebl/) file. Select "Save Elements"
from the File menu, or type Ctrl-S (Open-Apple-S on Mac).  The current
display parameters can also be saved from the File menu ("Save
Parameters").  Note that the comments in the configuration file are
lost in this process, so you may prefer to edit the file by hand.

To facilitate analyzing large libraries of recordings, shortcuts are
provided for iterating through the files in a directory.  Use Ctrl-N
and Ctrl-B to move to the next or previous file in the current
directory.

*** Pitch calculation

You can run the pitch tracking algorithm from within /chirp/ using the
"Calculate Pitch" menu item under "Analysis".  This is a
computationally intensive operation and the program will be
nonresponsive until it's finished.  On completion, the results will be
overlaid on the spectrogram as a series of white markers.  A separate
series of markers is shown for each analysis chain (see [[*Pitch%20tracking%20parameters][Pitch tracking
parameters]] for more information).  You can also load the results of a
pitch calculation from a /plg/ file using the "Open File" menu item,
or, if the /plg/ file has the same base name as the wave file, with
"Load Pitch Data".  In this case only a single set of markers will be shown.

**** Mask design considerations

For recordings with exceptional quality it may not be necessary to do
any masking.  Near-field recordings, obtained by placing a microphone
close to a nest or perch site,will tend to have less noise than
recordings obtained with a shotgun or parabolic microphone, but some
degree of masking may still be desirable if there is reverb or strong
stationary noise (i.e. with relatively constant spectrum).

Drawing good outlines is a bit of an art form, and you should expect
to spend a good amount of time ascending the learning curve. Each
species and recording setup will present its own challenges. It's also
important to fine-tune the parameters of the tracker, which are
discussed in [[*Pitch%20tracking%20parameters][Pitch tracking parameters]].  It may help to first read
[[*Pitch%20tracking%20theory][Pitch tracking theory]] for a fuller discussion of how the algorithm
works.

Generally speaking, the algorithm will have the most problems when
there are multiple ways of tracking through the spectrogram.  Imagine
starting at the beginning of your signal and trying to stay on top of
the ridge defined by the fundamental frequency of the signal.  If
there's noise in the signal that creates shortcuts, the algorithm
can't tell which is the true path and which isn't.  Broadband noise is
less of a problem than narrowband noise, which has a well-defined
pitch and represents a serious "temptation" for the tracker.  There
can also be issues when the pitch is being rapidly modulated, because
physical reverb and the inevitable limits in time-frequency resolution
can smear the power between closely spaced components.  

For example, in the recording we opened above, there's a rapid
downmodulation of the pitch that occurs around 110 ms.  The signal
reverberates slightly, leading to interference between the fundamental
frequency and the first overtone.  The lower frequency is stronger, and
a better fit to the harmonic template.  In many cases the tracker will
be able to find its way through correctly, but given the probabilistic
nature of the algorithm it may need some help.  And the lower the
signal to noise ratio of the recording, the more help it needs.

It's a good idea to start with a fairly broadband mask and trim it
down.  The more harmonics in the signal, the stronger the evidence for
the fundamental frequency (FF).  Even harmonics you can't see in the
spectrogram may be powerful enough to contribute positively.  On the
other hand, frequencies lower than the fundamental are only going to
interfere.  I usually follow closely below the FF and then take the
mask well above the highest visible harmonic, as shown below:

***** TODO screenshot of starting mask

Unfortunately, for this signal the tracker gets off the FF at a number
of points.  Several of the chains jump up to the higher harmonic, and
the variance of the estimate is high (indicated by the black symbols).
The first step I'll take is to subtract out the areas between the
harmonics where the spectrogram smears.  It's easy to do this by
drawing out that region with the mouse and then using the 'x' key to
subtract. As shown in the next figure, these regions can be entirely
enclosed within the mask, and it often doesn't take a lot to get the
algorithm back on track.  The variance is still a little bad in the
middle hairpin, but the central moment (see [[*Pitch%20tracking%20theory][Pitch tracking theory]]) is
still pretty much right on.

***** TODO screenshot of ending mask

More examples showing how masks can deal with other kinds of
interference are in the examples directory.  Sometimes no amount of
masking can produce a good pitch trace, or the mask may have to be
drawn so tightly that you might as well have traced the pitch
yourself.  Standards will vary depending on the application, but a good
general rule is that if you can't see the fundamental frequency
clearly, the recording probably needs to be excluded.

The shape of the mask is especially critical at the beginnings and
ends of the signals.  If the mask extends beyond the signal, the
tracker will generally bounce around until it hits the signal, and if
the initial guess is bad, it may have a hard time getting on track.
The variance will tend to be high where there is no signal, and it's
possible to postfilter the estimates to eliminate these points, but if
the mask is drawn badly, it may give a bad estimate and reduce the
variance (especially when the 'remask_likelihood' parameter is set).
A general rule is that if the mask is very restrictive in frequency
then it needs to be very precise temporally as well.  It's a good idea
to have the mask narrow toward the beginning and end of the signal,
rather than risk having it go past the beginning where you don't want
it to.

As a final note, the tracker parameters, including the spectrogram
resolution, can interact in a highly nonlinear fashion with your
masks.  A mask that works well with one set of parameters may perform
poorly with another set of parameters.  The parameters that control
the harmonic template are particularly sensitive.  A good strategy is
to first adjust the parameters until most of the recordings give good
estimates, then draw broadband masks, adjust the parameters again, and
then fine-tune the masks as a final step.


** cpitch

*** Overview

Although it's possible to calculate pitch traces in /chirp/, when
you've settled on a final set of parameters and masks, the /cpitch/
program is much more convenient for batch processing. It generates the
output files that are used in later stages of analysis, and which can
be read into third-party analysis software.  Usage is straightforward:

#+begin_src sh
cpitch -c <configfile> -m <maskfile> <wavfile>
#+end_src

The configuration file is the same on used by the /chirp/ GUI, and the
maskfile is the /ebl/ file generated by /chirp/.  Specifying a mask
file is optional.  The program writes to standard out (i.e. to the
console), so to store the output, redirect it into another file.  For example:

#+begin_src sh
cpitch -c chirp.cfg -m examples/KENB_MZ000162_sono_1_124570_134767_m12.ebl \
  examples/KENB_MZ000162_sono_1_124570_134767_m12.wav > examples/KENB_MZ000162_sono_1_124570_134767_m12.plg
#+end_src

*** Batch processing

It's expected that most users will have a large library of recordings
to analyze. Rather than manually run /cpitch/ for each file, it's a
good idea to set up a batch processing system, especially as this
opens up the possibility of running multiple analyses in
parallel. Many such systems are available.  The *chirp* package
supplies an example that uses scons (http://www.scons.org), a freely
available build system similar to Make.  The build process is
controlled by the SConstruct file in the examples subdirectory.  You
can place this file in any directory with /wav/ and /ebl/ files and
run the following command (you will need to have installed scons):

#+begin_src sh
scons -j N
#+end_src

where N is the number of processes to run simultaneously, something on
the order of the number of available processors.  By default, the
/cpitch/ processes will look for a file called 'chirp.cfg' in the
current directory, but you can set the location of this file in the
SConstruct file.  Scons will check files to determine if they've
changed; if you run the above command after editing some of the mask
files, only the recordings that have been edited will be reanalyzed.
The configuration file is also a dependency; if this is edited all the
recordings will have to be reanalyzed.

*** Pitch tracking theory

The pitch tracking algorithm used by *chirp* combines three separate
algorithms:

1. Time-frequency reassignment spectrographic analysis
2. Harmonic template matching
3. Bayesian particle filtering

Harmonic sounds are defined by a distribution of spectral energy with
peaks at integral multiples of some fundamental frequency.  On a
logarithmic scale, the harmonic peaks are separated by a constant
distance ($\theta, \theta + \log 2, \theta + \log 3,\ldots$ where
$\theta$ is the pitch or fundamental frequency).  An estimate of the
pitch can be obtained by calculating the spectrum on a logarithmic
frequency grid and cross-correlating it with a harmonic template.  For
nonstationary signals, the spectrum is typically calculated in short,
overlapping analysis windows, yielding a spectrogram or short-time
Fourier transform (STFT), and the pitch can be calculated in each
window ($\theta_t$) to observe how it changes in time.

An alternative to the STFT is the time-frequency reassignment
spectrogram (Auger and Flandrin, 1996), which can achieve arbitrarily
high spectral and temporal precision, though it is still subject to
the inevitable time-frequency tradeoff in resolution (i.e. when two or
more acoustic components are close or overlap spectrotemporally).  An
additional advantage to the reassignment spectrogram for this
application is that spectra can be calculated on a logarithmically
spaced frequency grid and thus directly compared to the harmonic
template.

The harmonic template is a ideal spectrum constructed from a
synethesized pulse train.  The Fourier transform of a this pulse train
has logarithmically spaced peaks, and when this spectrum is
cross-correlated against the signal spectrum the largest overlap will
occur when the template has been shifted so that its fundamental
frequency is at $\theta$.  However, the template will also match well
when the shift is equal to $\theta/2$, $2 \theta$, and so forth, an
error called pitch doubling.  For high-quality, noise-free recordings
the maximum of the cross-correlation will be at the true fundamental
frequency, but in field recordings there is often substantial noise at
low frequencies and interference from environmental sources, which can
obscure the fundamental frequency and lead to a large number of pitch
doubling errors.

As described by Wang et al. (2000), the harmonic template can be
adjusted to reduce these errors. The area under each peak of the
template is normalized and then scaled to decrease exponentially with
each successive peak, to reduce the contribution of higher harmonics.
Negative peaks can also be added to the template between the positive
ones.

The Bayesian particle filter also helps to deal with doubling/halving
errors by imposing a continuity constraint.  A full discussion of
particle filters is beyond the scope of this discussion; interested
readers can consult Meliza et al., "Pitch detection in songbirds: a
novel method for assessing vocal similarity" for more details.  In
this part of the algorithm, the cross-correlation of the spectrogram
with the harmonic template serves as a probability distribution, and
the amount that the pitch changes between frames is estimated by
calculating another cross-correlation, this time between neighboring
frames.  An optional smoothing step can be taken after the particle
filter is complete, to backtrace the best path through the probability
distribution.

*** Pitch tracking parameters

All adjustable parameters of the pitch tracking algorithm are set in a
configuration file.  An example, 'chirp.cfg', is provided in the root
directory of the package.  Each option and parameter is documented,
with some discussion of the effects on the analysis.  Consult this
file for the most up-to-date description of the parameters and their
effects.


** ccompare

*** Overview

The /ccompare/ program performs pairwise comparisons between
recordings.  Several different methods of comparison are supplied in
the *chirp* package. The pitch traces calculated by /cpitch/ can be used as
the basis for a dynamic time warping algorithm that provides excellent
performance for tonal signals.  A spectrographic cross-correlation
(SPCC) method is supplied, with an option for denoising the signals
using the masks created in /chirp/.

/Ccompare/ uses a plugin-based architecture, allowing users to extend
existing algorithms and write new ones.

*** Basic usage

As with the batch processing described previously, /ccompare/ operates
on all the recordings in the current directory.  It loads the
recordings (or a derived quantity like the pitch traces) into memory,
and then performs all the pairwise comparisons (or half of them, if
the comparison metric is symmetric).  It utilizes multiple processes
to increase speed on multicore machines.  For example, to compare the
example recordings using dynamic time warping of the pitch traces:

#+begin_src sh
ccompare -c ../chirp.cfg -m pitch_dtw -j 8 > pitch_comparisons.clg
#+end_src

As with scons, the '-j' parameter controls the number of processes
that will run in parallel. Like /cpitch/, the main output of the
program needs to be redirected into a file.  An indicator of the
analysis's progress will be output to the console.  If you have the
the progressbar python package installed, this will be a nice progress
bar.

The file output contains a number of comment lines, which start with
asterisks, and two tables.  The first is a list of all the recordings
the program analyzed, and a unique integer ID.  The second table can
be very long; it has fields for the ID of the reference and target
recordings, and any number of statistics for the comparison between
those recordings.  The number and meaning of the statistics will
depend on the comparison methods.

*** Spectrographic cross-correlation

A commonly used measure of acoustic similarity is known as
spectrographic cross-correlation (SPCC).  It was first introduced by
Clark et al. (1987), and consists of sliding two spectrograms across
each other and calculating the correlation coefficient at each lag.
If the spectrograms are similar, there will be some lag at which they
overlap well and the correlation coefficient will be high.  Typically
the SPCC is taken as the maximum value of the CC across all the
temporal lags.

There are a number of issues with using SPCC for comparing bioacoustic
recordings, especially ones obtained under noisy conditions.  On the
one hand, SPCC is extremely sensitive to differences in frequency and
rates of frequency modulation.  If one signal is only tens of Hz
higher than another, their spectrograms will fail to overlap.
Similarily, if two frequency-modulated signals change at slightly
different rates, they will fail to overlap except at a single time
point.  Thus, small differences in the frequency or duration of
vocalizations can lead to much lower SPCC values than would be
expected from visually examining the spectrograms.

On the other hand, SPCC values can be high when two signals share
similar overall spectra.  For example, if two recordings are obtained
in the presence of a persistent 1 kHz hum, there will be a constant
band across both spectrograms, leading to increased SPCC values.

In general, SPCC should be avoided whenever possible.  For some
signals (lacking strong tonal characteristics, for examples), or for a
preliminary analysis, it may still have some use.  *Chirp* includes a
basic SPCC algorithm, and an extended algorithm that can use masks in
/ebl/ files to mask out noise or restrict analysis to specific
temporal segments.  To use the basic algorithm on the example files:

#+begin_src sh
cpitch -c ../chirp.cfg -m spcc > spcc_comparisons.clg
#+end_src

And to use the extended algorithm:

#+begin_src sh
cpitch -c ../chirp.cfg -m masked_spcc > spccm_comparisons.clg
#+end_src

Both algorithms have options that can be set in the configuration
file.  The output is a single number, the peak correlation
coefficient, which ranges between 0.0 and 1.0, with higher numbers
indicating greater overlap.  

*** Dynamic time warping

The second comparison algorithm provided by /ccompare/ attempts to
ameliorate the two major issues with SPCC.  First, it's based on pitch
instead of the spectrogram.  For noisy signals, the experimenter has
an opportunity to visualize the effect of the noise on the pitch
calculation, and to try to minimize it.  A further effect of this
transformation is that small differences in pitch have a smaller
effect on the calculated similarity than large difference in pitch; in
contrast, with SPCC almost any difference in pitch has the same
effect.  Assuming that pitch is a relevant percept for the animal
under study, this ought to bring calculations more in line with
perceived differences.

Second, dynamic time warping (DTW) allows the two signals to stretch
and compress in time.  This means that two signals differing slightly
in duration or rate can be matched to each other, in contrast to a
cross-correlation, which does not allow any warping.  DTW was first
developed for use in speech recognition (Vintsyuk, 1971).  Briefly,
the algorithm consists of calculating a dissimilarity metric for every
pair of time points in the two signals.  In /ccompare/ this metric is
the Euclidean distance between the two pitch estimates.  This forms an
$N \times M$ matrix.  Then, using a "move cost matrix" that defines a
cost multiplier for making certain kinds of moves through the matrix,
the algorithm attempts to find the best (lowest cost) path through the
matrix.  The distance (or dissimilarity) between the two signals is
the total cost of moving along that path.  

For example, if two signals are identical, the diagonal of the matrix
will be zero, and the best path will be along the diagonal.  If two
signals are identical but have been warped slightly, then there will
be some path of zeros lying along or to either side of the diagonal.
As the signals differ in pitch, the total cost will increase,
indicating their dissimilarity.  The move cost matrix can be adjusted
to allow different degrees of warping.  The matrix consists one or
more sets of three number tuples ($x,y,d$), which define the cost
multiplier ($d$) associated with moving $x$ frames in one signal and
$y$ frames in the other.  The "standard" cost matrix is
[(1,0,1),(0,1,1),(1,1,1)], which allows any degree of warping.
For most signals this is not desirable, as it allows entire chunks of
one signal to be skipped.  The default setting in /ccompare/ is the
so-called "Itakura constraint", which allows no more than 1
consecutive frame to be skipped.  Signals can only be warped by a
factor of two with this constraint, which means that if they differ in
length by more than 2 times, the distance will be undefined.  To avoid
having missing values in the dataset, one can dynamically adjust the
cost matrix based on the difference in duration.  This algorithm
allows signals that differ greatly in length to be compared, but with
an increasing penalty for larger differences.  The distance measures
will be extremely large, but they will be finite, which can make later
clustering analyses much more tractable.


** csplitter

The /ebl/ files generated by /chirp/ can be used to extract signals of
interest from primary recordings.  Recordings can be rapidly split
using temporal intervals, and 2D spectrotemporal masks can be used to
extract the signals associated with those regions.  This is an
extremely effective method of filtering out noise from recordings,
because the masks provide much finer-grained control than traditional
bandpass filters.

** Utility programs

*** cplotpitch

The /cplotpitch/ script generates a PDF file with spectrograms and
overlaid pitch traces, for rapid inspection of signals and the
performance of the pitch tracking algorithm.  Run the script in the
directory with your wave and plg files. For the example files:

#+begin_src sh
cplotpitch -c ../chirp.cfg estimates.pdf
#+end_src

* Future directions

* Acknowledgements and References

